import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import os
import torch
import random

st.set_page_config(page_title="êµ¬í¬ì´ˆë“±í•™êµ AI ì±—ë´‡")

# CUDA ì„¤ì •
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_model():
    model = SentenceTransformer('kykim/bert-kor-base')
    model.to(torch.device(DEVICE))
    return model

model = load_model()

# ì§€ì‹ íŒŒì¼ ë¡œë“œ (4ë¬¸ì¥)
def load_knowledge(file_path="population_busan.txt"):
    if not os.path.exists(file_path):
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

FULL_KNOWLEDGE = load_knowledge()

# ì‹œë„ ê´€ë ¨ íƒœê·¸
TAG_SENTENCES = [
    "ì‹œë„", "ì „êµ­", "ëŒ€í•œë¯¼êµ­", "ìš°ë¦¬ë‚˜ë¼",
    "ê²½ê¸°ë„", "ì„œìš¸", "ê²½ìƒë‚¨ë„", "ê²½ìƒë¶ë„", "ëŒ€êµ¬",
    "ì¶©ì²­ë‚¨ë„", "ì¸ì²œ", "ì „ë¼ë‚¨ë„", "ì „ë¶íŠ¹ë³„ìì¹˜ë„", "ì¶©ì²­ë¶ë„",
    "ê°•ì›ë„", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ì œì£¼ë„", "ì„¸ì¢…"
]

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "history" not in st.session_state:
    st.session_state["history"] = []

# UI
st.title("ğŸ•Šï¸ êµ¬í¬4-1ë°˜ ì´ˆë“± AI ì±—ë´‡")
st.markdown("<h3 style='color:#0078D7;'>ì¸êµ¬, ë©´ì  ë°ì´í„°ë¥¼ ë‹´ê³  ìˆëŠ” ì±—ë´‡ì´ì—ìš”</h3>", unsafe_allow_html=True)

# ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ì´ˆê¸°í™”"):
    st.session_state["history"] = []
    st.success("ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")

user_input = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?")

# FAISS ì¸ë±ìŠ¤ êµ¬ì¶• í•¨ìˆ˜
def build_faiss_index(sentences):
    embeddings = model.encode(sentences, convert_to_numpy=True, device=DEVICE)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    return index, sentences

# ì§ˆë¬¸ ì²˜ë¦¬
if st.button("ì§ˆë¬¸í•˜ê¸°") and user_input:
    if not FULL_KNOWLEDGE or len(FULL_KNOWLEDGE) < 4:
        st.error("ì§€ì‹ íŒŒì¼ì— í•„ìš”í•œ ë¬¸ì¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    else:
        # ë©´ì  ì§ˆë¬¸ì¼ ê²½ìš° 4ë²ˆì§¸ ë¬¸ì¥ ì¶œë ¥
        if "ë©´ì " in user_input:
            matched_answer = random.choice([FULL_KNOWLEDGE[3]])

        # ì‹œë„ ê´€ë ¨ ì§ˆë¬¸ì¼ ê²½ìš° 3ë²ˆì§¸ ë¬¸ì¥ ì¶œë ¥
        elif any(tag in user_input for tag in TAG_SENTENCES):
            matched_answer = FULL_KNOWLEDGE[2]

        # ì¼ë°˜ ì§ˆë¬¸: 1~2ë²ˆì§¸ ë¬¸ì¥ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰
        else:
            knowledge_subset = FULL_KNOWLEDGE[:3]  # 0, 1, 2ë²ˆì§¸ ë¬¸ì¥ë§Œ
            index, searchable_sentences = build_faiss_index(knowledge_subset)

            query_vec = model.encode([user_input], convert_to_numpy=True, device=DEVICE)
            D, I = index.search(np.array(query_vec), k=1)

            if D[0][0] > 500.0:
                matched_answer = "ì§ˆë¬¸ì´ ì˜ ì´í•´ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. 6Quizë¥¼ í™œìš©í•´ë´ìš”!"
            else:
                matched_answer = searchable_sentences[I[0][0]]

        st.markdown(f"**ì±—ë´‡:** {matched_answer}")
        st.session_state["history"].insert(0, (user_input, matched_answer))

# ì´ì „ ì§ˆë¬¸ ê¸°ë¡
if st.session_state["history"]:
    st.markdown("---")
    st.subheader("ğŸ“œ ì´ì „ ì§ˆë¬¸ ê¸°ë¡")
    for idx, (prev_q, prev_a) in enumerate(st.session_state["history"], 1):
        with st.expander(f"Q{idx}: {prev_q}", expanded=False):
            st.markdown(prev_a)
